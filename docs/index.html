<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>4641 Project Group 6</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">4641 Project Group 6</h1>
      <h2 class="project-tagline">A responsive web app for recognition of vegetables and fruits</h2>
      <h2 class="project-tagline">Group 6: Jiajun Mao, Yue Pan, Wei Xin, Lisha Yang, Tongshu Yang</h2>
      <a href="#" class="btn">View on GitHub</a>
      <a href="#" class="btn">Download .zip</a>
      <a href="#" class="btn">Download .tar.gz</a>
    </section>
    <iframe src="https://delicat-monsieur-08095.herokuapp.com" style="position:relative; left:1em" scrolling="no" width="1800" height="990"></iframe>
    <section class="main-content">
      <ol>
        <p>*First time loading may take up to 2 minutes</p>
        <h2 class="code-line" data-line-start=6 data-line-end=7><a id="1_Introduction_6"></a>1  <strong>Introduction</strong></h2>
        <h3 class="code-line" data-line-start=8 data-line-end=9><a id="11_Motivation_8"></a>1.1 Motivation</h3>
        <p class="has-line-data" data-line-start="10" data-line-end="11">During this special time of pandemic, cooking at home has become increasingly popular among the younger generation, but it also raised a question, which is how to cook healthily. Unlike other packed products, the raw materials we can buy like meat and vegetables don’t come with nutrition facts. There are some applications or websites that help track what you eat. However, most of them require manually typing in the names of materials which is tedious and time-consuming. Kitchen Alchemy is here to help. You only need to upload a picture of what you eat to our website, we will do everything for the rest. In the end, we will also provide you a graph which helps visualize the nutrient facts.</p>
        <h3 class="code-line" data-line-start=12 data-line-end=13><a id="12_Project_overview_12"></a>1.2 Project overview</h3>
        <p class="has-line-data" data-line-start="14" data-line-end="15">Our project, Kitchen Alchemy, uses neural networks for image recognition.Different from the traditional tracking process, our product provides all necessary nutrition facts conveniently and efficiently. With our program, people may know more about the nutrient facts of their food and develop healthier eating habits.</p>
        <h2 class="code-line" data-line-start=16 data-line-end=17><a id="2_Dataset_Collecting_and_processing_16"></a>2 <strong>Dataset Collecting and processing</strong></h2>
        <h3 class="code-line" data-line-start=18 data-line-end=19><a id="21_Dataset_Description_18"></a>2.1 Dataset Description</h3>
        <h4 class="code-line" data-line-start=20 data-line-end=21><a id="211_Dataset_Source_20"></a>2.1.1 Dataset Source</h4>
        <p class="has-line-data" data-line-start="22" data-line-end="23">We started with multiple datasets from Kaggle, ImageNet, and Google data. After comparing different datasets, we chose Fruit 360 from Kaggle as our main raw dataset. [1] Fruits and vegetables were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded. Due to the variation of the light condition and background, they decided to extract the object from the background. The Fruit 360 dataset was built on 02/25/2017 but has been actively updated. We used the current newest version which is Version: 2020.05.18.0.</p>
        <p class="has-line-data" data-line-start="24" data-line-end="25"><img src="image_1.png" alt="image alt text"></p>
        <p class="has-line-data" data-line-start="26" data-line-end="27">The nutrient facts dataset was obtained from NSDA and some other datasets found online. For some of the fruits that were not in the dataset, the values were manually searched and inputted.</p>
        <h4 class="code-line" data-line-start=28 data-line-end=29><a id="212_Dataset_characteristics_the_original_dataset_28"></a>2.1.2 Dataset characteristics (the original dataset)</h4>
        <table>
          <tr>
          <td>Total number of images</td>
          <td>Training set size</td>
          <td>Test set size</td>
          <td>image size</td>
          <td>Total number of classes</td>
          </tr>
          <tr>
          <td>90483</td>
          <td>67692</td>
          <td>22688</td>
          <td>100x100 pixels</td>
          <td>131</td>
          </tr>
          </table>
        <h3 class="code-line" data-line-start=48 data-line-end=49><a id="22_Data_preprocessing_48"></a>2.2 Data preprocessing</h3>
        <p class="has-line-data" data-line-start="50" data-line-end="51">Since there are some fruits in the dataset that are not commonly seen in daily life, we decided to merge those varieties into one class such as group Kumquats with Orange so that the training process would be easier. After cleaning, we have 28 classes in total for both training and testing. Due to combing different classes together, depending on the class the number of images in one class vary from 103 to 11*103.</p>
        <p class="has-line-data" data-line-start="52" data-line-end="53">After cleaning our dataset, we need to connect the nutrient facts table to the classes. In the nutrient chart, we have 12 categories associated with each class.</p>
        <p class="has-line-data" data-line-start="54" data-line-end="55"><img src="image_2.png" alt="image alt text"></p>
        <h2 class="code-line" data-line-start=56 data-line-end=57><a id="3_Method_56"></a>3 <strong>Method</strong></h2>
        <h3 class="code-line" data-line-start=58 data-line-end=59><a id="31_Algorithms_and_Models_58"></a>3.1 Algorithms and Models</h3>
        <h4 class="code-line" data-line-start=60 data-line-end=61><a id="311_Convolutional_Neural_Network__A_Transfer_Learning_Approach_60"></a>3.1.1 Convolutional Neural Network – A Transfer Learning Approach</h4>
        <p class="has-line-data" data-line-start="62" data-line-end="63">When using a CNN model to complete a typical image classification task, there are usually two approaches. One can build a CNN network from scratch, or use transfer learning from a pre-trained model. We chose the latter approach for a few reasons. First, the transfer learning approach can take advantage of the pre-trained model in terms of structure, weights, and hyperparameters. Second, training a transferred model is more time-efficient than a new model. Transferred models often have less weights, which helps bring down the hardware requirement for training a network.</p>
        <p class="has-line-data" data-line-start="64" data-line-end="65">In our project, Python libraries Tensorflow as Keras are used in our protypying. With the help of these libraries, manipulations of the network are much easier. We finally chose the ImageNet Contest winning model MobileNetV2 as our starting point. MobileNetV2 has considerably less model size and weights compared to other popular CNN like VGG16/19 and InceptionV3. To fit with our task, the last layer of the original MobileNetV2 (dense layer with 1000 outputs nodes) is removed and replaced with our custom layers:</p>
        <ul>
        <li class="has-line-data" data-line-start="66" data-line-end="68">
        <p class="has-line-data" data-line-start="66" data-line-end="67">A Global Average Pooling to shrink network size</p>
        </li>
        <li class="has-line-data" data-line-start="68" data-line-end="70">
        <p class="has-line-data" data-line-start="68" data-line-end="69">A Dense Layer with 512 outputs nodes with activation function ReLU</p>
        </li>
        <li class="has-line-data" data-line-start="70" data-line-end="72">
        <p class="has-line-data" data-line-start="70" data-line-end="71">A Dropout layer with rate 0.5 to prevent overfitting</p>
        </li>
        <li class="has-line-data" data-line-start="72" data-line-end="74">
        <p class="has-line-data" data-line-start="72" data-line-end="73">A final dense layer with 28 output nodes (classes for predictions)</p>
        </li>
        </ul>
        <p class="has-line-data" data-line-start="74" data-line-end="75">To further prevent overfitting, we also added two L2 Regularizations to the two dense layers.</p>
        <p class="has-line-data" data-line-start="76" data-line-end="77">Meanwhile, augmentations were applied to the training dataset. With the ImageDataGenerator class from Keras, the actual images fed into the network may include random shifts in zoom range, width-to-height ratio, etc. This helps the model better generalize to real-world images.</p>
        <p class="has-line-data" data-line-start="78" data-line-end="79">With this network structure, training takes about 50 minutes for a total of 20 epochs on a GTX 1060 6GB Max-Q. At the end, the model is able to reach an accuracy of 96% and a cross-entropy loss of 0.38 on the validation dataset. Afterwards, we tested our trained model using images outside of the validation dataset. It performed within our expectations.</p>
        <p class="has-line-data" data-line-start="80" data-line-end="81"><img src="image_3.png" alt="image alt text"></p>
        <h4 class="code-line" data-line-start=82 data-line-end=83><a id="312_FasterRCNN_and_Regional_Proposal_Network_82"></a>3.1.2 Faster-RCNN and Regional Proposal Network</h4>
        <p class="has-line-data" data-line-start="84" data-line-end="85">To offer better user experiences by allowing users to directly take pictures of their fridge or arranged cooking materials to obtain the nutritious value, we decided to also implement a segmentation layer that will serve as an input to the recognition network for nutrientritious values. Since the user-data is photo-based, there are no real-time requirements for image segmentation. Hence an improved branch of the RCNN network - Faster-RCNN was implemented. Faster-RCNN utilizes Anchor Boxes and Regional Proposal Network, which consists of:</p>
        <ul>
        <li class="has-line-data" data-line-start="86" data-line-end="88">
        <p class="has-line-data" data-line-start="86" data-line-end="87">A convolution layer with kernel size of 3x3 and filter size of 512</p>
        </li>
        <li class="has-line-data" data-line-start="88" data-line-end="90">
        <p class="has-line-data" data-line-start="88" data-line-end="89">An output linear classifier with 4*k outputs</p>
        </li>
        <li class="has-line-data" data-line-start="90" data-line-end="92">
        <p class="has-line-data" data-line-start="90" data-line-end="91">An output sigmoid regressor with 1*k outputs (k being the number of anchor boxes)</p>
        </li>
        </ul>
        <p class="has-line-data" data-line-start="92" data-line-end="93">in parallel to CNN feature extraction to come up with possible ROIs that contain images of food. For the feature extraction network that serves as both the input to the RPN and the ROI pooling layer we used InceptionResnetV2 that should provide us with decent accuracy while not incurring heavy computation cost. However, due time constraint we were unable to integrate the Faster-RCNN and RPN into our project’s final submission.</p>
        <h2 class="code-line" data-line-start=94 data-line-end=95><a id="4_Results_and_Discussion_94"></a>4 <strong>Results and Discussion</strong></h2>
        <h3 class="code-line" data-line-start=96 data-line-end=97><a id="41_Results_presentation_96"></a>4.1 Results presentation</h3>
        <p class="has-line-data" data-line-start="98" data-line-end="99">On the sidebar, a “Choose File” button appears under “View Your Report”, That’s where the picture of the fruit is uploaded.</p>
        <p class="has-line-data" data-line-start="100" data-line-end="101"><img src="image_4.png" alt="image alt text"></p>
        <p class="has-line-data" data-line-start="102" data-line-end="103">After a few seconds, the predicted result would appear on the right with the data visualization on the bottom.</p>
        <p class="has-line-data" data-line-start="104" data-line-end="105"><img src="image_5.png" alt="image alt text"><img src="image_6.png" alt="image alt text"></p>
        <h3 class="code-line" data-line-start=106 data-line-end=107><a id="42_Limitation_and_Future_work_106"></a>4.2 Limitation and Future work</h3>
        <p class="has-line-data" data-line-start="108" data-line-end="109">Like most CNN models, our model has limitations with regard to prediction accuracy for certain types of fruit that are similar in shape and color. Infrequently, the model predicts a red apple as a tomato, or an orange as a potato. This is mainly due to the limitation of the dataset used in training. As mentioned in the dataset section, images in this dataset are taken from different perspectives of the same fruit. As a result, the color, texture, shape etc. learned from the dataset are limited to that particular fruit/vegetable and sometimes do not generalize well to various characteristics of a real-world object.</p>
        <h3 class="code-line" data-line-start=110 data-line-end=111><a id="43_Conclusion_110"></a>4.3 Conclusion</h3>
        <p class="has-line-data" data-line-start="112" data-line-end="113">Overall, since the main limitation we are facing right now is the number and variety of our datasets. The next step that we are looking to is gathering more pictures for our classes. This may help improve the accuracy of current classes. Beyond that, we will also try some other models with more parameters and see whether or not it will help with performance. Moreover, we are also considering adding or finishing some other features that we mentioned in the proposal such as recommendations.</p>
        <h2 class="code-line" data-line-start=114 data-line-end=115><a id="5_References_114"></a>5 <strong>References</strong> </h2>
        <p class="has-line-data" data-line-start="116" data-line-end="117">[1] Horea Muresan, Mihai Oltean, Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.</p>
        <p class="has-line-data" data-line-start="118" data-line-end="119">[2] Song, Y., Elkahky, A. M., &amp; He, X. (2016). Multi-Rate Deep Learning for Temporal Recommendation. <em>Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR 16</em>. doi: 10.1145/2911451.2914726</p>
        <p class="has-line-data" data-line-start="120" data-line-end="121">[3] Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2017). ImageNet classification with deep convolutional neural networks. <em>Communications of the ACM,</em> <em>60</em>(6), 84-90. doi:10.1145/3065386</p>
        <p class="has-line-data" data-line-start="122" data-line-end="123">[4] Xie, S., Kirillov, A., Girshick, R., &amp; He, K. (2019). Exploring Randomly Wired Neural Networks for Image Recognition. <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>. doi: 10.1109/iccv.2019.00137</p>
        <h2>6 <strong>Contribution</strong></h2>
        <p>Jiajun Mao: Segmentation</p>
        <p>Yue Pan: Model training and tuning</p>
        <p>Wei Xin: Website development, User Interface</p>
        <p>Lisha Yang: Nutrient Fact chart, User Interface</p>
        <p>Tongshu Yang: Data Cleaning</p>
    </section>

  </body>
</html>
